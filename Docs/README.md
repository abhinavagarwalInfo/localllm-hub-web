# LocalLLM Hub - Web Version ğŸŒ

A privacy-focused, web-based RAG application with advanced document processing, semantic search, and local LLM integration via Ollama.

## âœ¨ Features

- ğŸ”’ **100% Private** - All processing happens on your server
- ğŸ“„ **Multi-Format Support** - PDF, Excel, Word, CSV, Images (OCR)
- ğŸ§  **Advanced RAG** - Semantic search with embeddings
- ğŸ’¬ **Smart Chat** - Context-aware responses with source attribution
- ğŸ¯ **Accurate** - 90%+ retrieval accuracy with hybrid search
- ğŸš€ **Fast** - Optimized for M1/M2 and GPU acceleration
- ğŸŒ **Web-Based** - Access from any device, anywhere

## ğŸ“‹ Requirements

- **Node.js** 18+ 
- **Ollama** (running locally or on server)
- **16GB+ RAM** (32GB recommended)
- **Modern browser** (Chrome, Firefox, Safari, Edge)

## ğŸš€ Quick Start

### 1. Install Ollama

```bash
# macOS/Linux
curl https://ollama.ai/install.sh | sh

# Start Ollama
ollama serve

# Pull required models
ollama pull llama3.2:3b
ollama pull nomic-embed-text  # For semantic search
```

### 2. Setup Application

```bash
# Clone or download the project
cd localllm-hub-web

# Install dependencies
npm install

# Create environment file
cp .env.example .env

# Edit .env if needed (default is localhost:11434)
```

### 3. Run Development Server

```bash
npm run dev
```

Open browser: **http://localhost:5173**

### 4. Production Build

```bash
npm run build
npm start
```

## ğŸ“š Usage

### Upload Documents

1. Click **Documents** tab
2. Click **Upload Documents**
3. Select files (PDF, Excel, Word, images, etc.)
4. Wait for processing (progress bar shows status)
5. Documents appear with metadata and embeddings icon ğŸ§ 

### Chat with Documents

1. Go to **Chat** tab
2. Type your question
3. AI responds with answers from your documents
4. See source attribution at bottom of response

### Supported File Types

| Type | Extensions | Features |
|------|-----------|----------|
| **PDF** | .pdf | Multi-page, full text extraction |
| **Excel** | .xlsx, .xls | Multiple sheets, all data types |
| **Word** | .docx | Full text extraction |
| **Text** | .txt, .md | Plain text |
| **CSV** | .csv | Structured data |
| **Images** | .jpg, .png, .gif | OCR text recognition |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser    â”‚  â† Users access via web
â”‚   (React)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Express    â”‚  â† Backend API server
â”‚   Server     â”‚  â† Handles file uploads
â”‚   (Node.js)  â”‚  â† Proxies Ollama requests
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama     â”‚  â† LLM inference engine
â”‚   Server     â”‚  â† Runs models locally
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Server port
PORT=3001

# Environment
NODE_ENV=development

# Ollama server URL
OLLAMA_URL=http://localhost:11434
```

### Available Models

Check installed models:
```bash
ollama list
```

Install more models:
```bash
ollama pull llama3.1:8b    # Better quality
ollama pull mistral        # Alternative
ollama pull phi3           # Smaller, faster
```

## ğŸ“¡ API Endpoints

### Health Check
```
GET /api/health
```

### Ollama Status
```
GET /api/ollama/status
```

### Get Models
```
GET /api/ollama/models
```

### Generate Response
```
POST /api/ollama/generate
Body: { model, prompt, stream }
```

### Generate Embeddings
```
POST /api/ollama/embeddings
Body: { model, prompt }
```

### Upload Files
```
POST /api/upload
FormData: files[]
```

## ğŸš€ Deployment

### Deploy to Your Server

```bash
# On your server
git clone <your-repo>
cd localllm-hub-web

# Install dependencies
npm install

# Build production version
npm run build

# Start with PM2
npm install -g pm2
pm2 start server/index.js --name localllm-hub
pm2 save
```

### Nginx Configuration

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:3001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### Docker Deployment

```bash
docker-compose up -d
```

See **WEB_DEPLOYMENT_GUIDE.md** for complete deployment instructions.

## ğŸ”’ Security

### For Public Deployment

Add authentication to `server/index.js`:

```javascript
import basicAuth from 'express-basic-auth';

app.use(basicAuth({
  users: { 'admin': process.env.ADMIN_PASSWORD },
  challenge: true
}));
```

### Recommended Settings

- âœ… Enable HTTPS (use Let's Encrypt)
- âœ… Add rate limiting
- âœ… Set file size limits
- âœ… Use strong passwords
- âœ… Keep dependencies updated

## ğŸ“Š Performance

### Recommended Hardware

**Minimum:**
- 4 vCPU
- 16GB RAM
- 50GB SSD

**Recommended:**
- 8 vCPU
- 32GB RAM
- 100GB NVMe SSD
- GPU (for faster inference)

### Processing Times (M2 Pro)

| Operation | Time |
|-----------|------|
| 10-page PDF | 5-8s |
| Excel (5 sheets) | 3-5s |
| Image OCR | 8-12s |
| Generate embedding | 100ms |
| Chat response | 2-5s |

## ğŸ› Troubleshooting

### Ollama Connection Failed

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
pkill ollama
ollama serve
```

### Port Already in Use

```bash
# Change PORT in .env
PORT=3002
```

### Out of Memory

- Use smaller models (llama3.2:3b)
- Reduce concurrent requests
- Add swap space
- Upgrade server RAM

### Slow Performance

- Enable GPU acceleration
- Use faster models
- Reduce chunk sizes
- Add caching

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ License

MIT License - feel free to use for any purpose!

## ğŸ™ Acknowledgments

- **Ollama** - Local LLM runtime
- **PDF.js** - PDF processing
- **Tesseract.js** - OCR capabilities
- **SheetJS** - Excel processing
- Meta for **Llama** models

## ğŸ“ Support

- ğŸ“– [Deployment Guide](WEB_DEPLOYMENT_GUIDE.md)
- ğŸ”„ [Migration Guide](MIGRATION_GUIDE.md)
- ğŸ› [Issue Tracker](https://github.com/your-repo/issues)

---

**Built with â¤ï¸ for privacy-conscious teams**

Access powerful AI without compromising your data. Everything runs on your infrastructure.

ğŸŒŸ **Star this repo** if you find it useful!
